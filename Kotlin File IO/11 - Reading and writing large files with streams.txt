When dealing with large files, it's essential to use streams to efficiently read and write data in chunks instead of loading the entire file into memory. Below is an example of how to read a large text file and write its content to another file using streams in Kotlin:

```kotlin
import java.io.*

fun main() {
    val sourceFileName = "large_source.txt"
    val destinationFileName = "large_destination.txt"

    try {
        val sourceFile = File(sourceFileName)
        val destinationFile = File(destinationFileName)

        val inputStream = BufferedReader(FileReader(sourceFile))
        val outputStream = BufferedWriter(FileWriter(destinationFile))

        val buffer = CharArray(1024) // Adjust buffer size as needed
        var charsRead: Int

        while (inputStream.read(buffer).also { charsRead = it } != -1) {
            outputStream.write(buffer, 0, charsRead)
        }

        inputStream.close()
        outputStream.close()

        println("Large file copied successfully from $sourceFileName to $destinationFileName.")
    } catch (e: IOException) {
        println("Error copying the file: ${e.message}")
    }
}
```

In this example:

1. We specify the name of the source file (`large_source.txt`) and the destination file (`large_destination.txt`) you want to copy the data to. Make sure the source file exists in the same directory as the Kotlin source file.

2. We use a `try-catch` block to handle any potential exceptions that might occur during file copying.

3. Inside the `try` block, we create `File` objects representing the source and destination files.

4. We create `BufferedReader` and `BufferedWriter` to read from the source file and write to the destination file, respectively.

5. We use a buffer of size 1024 characters to read data from the source file in chunks. You can adjust the buffer size based on your requirements.

6. We use a `while` loop to read data from the source file in chunks using `inputStream.read(buffer)` and write the same data to the destination file using `outputStream.write(buffer, 0, charsRead)`.

7. Finally, we close both the `inputStream` and `outputStream` to save the changes and release any system resources.

By using streams and reading/writing data in chunks, this approach can efficiently handle large files without consuming excessive memory.